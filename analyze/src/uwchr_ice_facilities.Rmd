---
title: "Exploratory Data Analysis (EDA) of Immigration and Customs Enforcement (ICE) Detention Facilities, 2016--2019"
subtitle: "Freedom of Information Act (FOIA) Request Records"
author: Thiago Marques
-institute: University of Washington Center for Human Rights
date: "January 6, 2020"
output:
  word_document: default
  pdf_document: default
---

# Introduction
Interior immigration enforcement is receiving increased attention from researchers in collaboration with activists and advocacy groups. The University of Washington Center for Human Rights answers the following research questions in partnership with such local social movements. The research questions in this project describe and explain the following associations:

* 1. What is the influence of detention center capacity on immigration law enforcement practices?
* 2. What is the influence of detention center capacity on immigration bail bonds?


This .Rmd file performs data wrangling tasks and exploratory analyses of administrative records on detention facilities spanning the years 2016--2019. Data were obtained from the United States Department of Homeland Security (DHS) federal law enforcement agency, US Immigration and Customs Enforcement (ICE) via Freedom of Information Act (FOIA) requests submitted by the University of Washington Center for Human Rights.
 
# Data wrangling
I first clear the working memory, load user-created packages, and import data with the `R` programming language and software environment for statistical computing and graphics. I tidy these data to then facilitate their transformation and visualization. 

## Initiate environment
```{r install_packages, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# clear memory
rm(list=ls())

# check current library paths
.libPaths() # [1] "C:/Users/marqu/Documents/R/win-library/3.6"
# [2] "C:/Program Files/R/R-3.6.2/library"

# install and load packages
library(pander) # creates tables
library(tidyverse) # easily install and load the 'Tidyverse' set of packages
library(tidycensus)
library(ipumsr)
library(shiny)
library(DT)
library(scales)
library(stringr)
library(lavaan)
library(forcats)
library(lubridate)
library(modelr)
```

Next, I will read three different data frames into `R`. The first data frame contains facility-level information on average daily population, X, Y, and Z. The second data frame contains individual-level arrest records. These data come from the U.S. Immigration and Customs Enforcement (ICE) agency responsible for interior immigration enforcement, and is  documented in SOURCE. Derived from the US Census Bureau American Community Survey (ACS), the third data frame comprises information on economic, social, and demographic characteristics of counties. 

## Import data
```{r read_data, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# check working directory
getwd() # "C:/Users/marqu/Google Drive/thiagm@uw.edu 2018-12-22 02_24/Academic/Graduate School/University of Washington/Research/Projects/ICE"

# set working directory
setwd("C:/Users/marqu/Google Drive/thiagm@uw.edu 2018-12-22 02_24/Academic/Graduate School/University of Washington/Research/Projects/ICE")

# load foia-ice rectangular data
  # read facilities data
  df_facilities <- ice_facilities_data <- read_delim(
    "C:/Users/marqu/git/ice-facilities/analyze/input/facil-list.csv.gz", 
    delim = "|") 
                    
  # read arrests data
  df_arrests <- ice_arrests_data <- 
      read_delim("C:/Users/marqu/git/ice-ero-lesa/us/clean/output/arrests.csv.gz", 
                 delim = "|")
    
    
# load (tidy)census decennial and inter-decennial rectangular data
    #vt <- get_decennial(
#  geography,
#  year = 2010,
#  geometry = 
#
#
# = "county", 
#             variables = c(medincome = "B19013_001"), 
#             state = "VT",
# year =
```

```{r structure, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# check number of rows and columns
nrow(df_facilities)
ncol(df_facilities)

# check structure of data
str(df_facilities) # no factor variables
glimpse(df_facilities)

# check beginning and end of data
head(df_facilities[, c(6:7, 10)])
tail(df_facilities[, c(6:7, 10)])

# check variable names
names(df_facilities)

# standardize variable names
#names(df_facilities) <- tolower(names(df_facilities))
#names(df_facilities)

```

## Data transformation
I will solve the vast majority of my data manipulation challenges in this section. Here, I will recode my variables of interest. It is important to note that I will be replacing the unusual values (i.e, not in universe, unknown) with missing values (i.e., NA in R).

### Select variables
#### Detention location
Detention facilities are uniquely identified by a code (`detloc`). Other geographic variables include facility name, street address, city, county, state, and ZIP code. Facilities are also located within an area of responsibility (AOR).

```{r detloc, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# detloc
# check levels
class(df_facilities$detloc)

df_facilities %>%
  count(detloc)

# verify unique IDs
df_facilities %>%
  count(detloc) %>%
  filter(n > 1) # rows are unique except for "redacted" obs (n = 207)
                # redacted obs are type == "ORR"

df_facilities %>%
  summarise(count = n_distinct(detloc)) # 1479 unique obs

df_facilities %>% 
  filter(detloc != "Redacted") %>% # filter out redacted obs
  summarise(count = n_distinct(detloc)) # 1478 unique obs

# convert to factor
df_facilities <- df_facilities %>%
  mutate(detloc = as.factor(detloc))
```

#### Facility name
```{r name, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# name
# check levels
class(df_facilities$name)
class(df_facilities$`name`)
class(df_facilities$"name")

df_facilities %>%
  count(name)
df_facilities %>%
  count(`name`)
df_facilities %>%
  count("name")

# verify unique IDs
df_facilities %>%
  count(name) %>%
  filter(n > 1) # facility names are not unique

df_facilities %>%
  summarise(count = n_distinct(name)) # 1407 unique facility names
```

#### Facility street address
```{r address, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# address
# check levels
class(df_facilities$address)
class(df_facilities$`address`)
class(df_facilities$"address")

df_facilities %>%
  count(address)
df_facilities %>%
  count(`address`)
df_facilities %>%
  count("address")

# verify unique IDs
df_facilities %>%
  count(address) %>%
  filter(n > 1)

df_facilities %>%
  summarize(count = n_distinct(address)) # 1416 facility addresses
```

#### Facility state
```{r state, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# state
# check levels
class(df_facilities$state)
class(df_facilities$`state`)
class(df_facilities$"state")

df_facilities %>%
  count(state) # 56 rows
df_facilities %>%
  count(`state`) # 56 rows
df_facilities %>%
  count("state") # 1685 rows
table(df_facilities$state)

# plot levels
ggplot(df_facilities, aes(state)) + 
  geom_bar()  +
  theme(axis.text.x = element_text(angle = 90, size = 8))

# view states with a bar chart
df_facilities %>%
  mutate(state = state %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(state)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# convert to factor
df_facilities <- df_facilities %>%
  mutate(state = as.factor(state))

# check proportions
table(df_facilities$state)
tab1 <- table(df_facilities$state)
prop.table(tab1)
df_facilities %>%
  count(state)
```

#### Facility city
```{r city, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# city
# check levels
class(df_facilities$city)
class(df_facilities$`city`)
class(df_facilities$"city")

df_facilities %>%
  count(city)
df_facilities %>%
  count(`city`)
df_facilities %>%
  count("city")

# verify unique obs
df_facilities %>%
  count(city) %>%
  filter(n > 1) # many duplicates

df_facilities %>%
  summarize(count = n_distinct(city))

# concatenate facility city and state columns
df_facilities <- df_facilities %>% 
  unite("city", c(city,state), sep = ", ", remove = FALSE, na.rm = FALSE) %>%
  mutate(city = as.factor(city))

# plot levels with a bar chart
df_facilities %>%
  mutate(city = city %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(city)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))
```

#### Facility ZIP code
```{r zip, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# zip
# check levels
class(df_facilities$zip)
class(df_facilities$`zip`)
class(df_facilities$"zip")

df_facilities %>%
  count(zip) # 1257 rows
df_facilities %>%
  count(`zip`) # 1257 rows
df_facilities %>%
  count("zip") # 1685 rows

# verify unique obs
df_facilities %>%
  count(zip) %>%
  filter(n > 1) # obs not unique

# plot levels with a bar chart
df_facilities %>%
  mutate(zip = zip %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(zip)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# check proportions
table(df_facilities$zip)
tab1 <- table(df_facilities$zip)
prop.table(tab1)
df_facilities %>%
  count(zip)
```

#### Facility area of responsibility (AOR)
```{r aor, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# aor
# check levels
class(df_facilities$aor)
class(df_facilities$`aor`)
class(df_facilities$"aor")

df_facilities %>%
  count(aor) # 24 rows
df_facilities %>%
  count(`aor`) # 24 rows
df_facilities %>%
  count("aor") # 1685 rows
table(df_facilities$aor)

# plot number of facilities within AORs
df_facilities %>%
  mutate(aor = aor %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(aor)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# convert to factor
df_facilities <- df_facilities %>%
  mutate(aor = as.factor(aor))

# check proportions
table(df_facilities$aor)
tab1 <- table(df_facilities$aor)
prop.table(tab1)
df_facilities %>%
  count(aor)
```

#### Facility type
```{r type, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# type                   
# check levels
class(df_facilities$type)
class(df_facilities$`type`)
class(df_facilities$"type")

df_facilities %>%
  count(type) # 8 rows
df_facilities %>%
  count(`type`) # 8 rows
df_facilities %>%
  count("type") # 1685 rows
table(df_facilities$type)

# plot levels with a bar chart
df_facilities %>%
  mutate(type = type %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(type)) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# convert to factor
df_facilities <- df_facilities %>%
  mutate(type = as.factor(type))

# check proportions
table(df_facilities$type)
tab1 <- table(df_facilities$type)
prop.table(tab1)
df_facilities %>%
  count(type)
```

#### Average daily population (ADP)
I will calculate summary statistics for the average daily population (ADP). I also plot their distributions.

```{r adp, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}
# fy18_adp
summarize(df_facilities, `Fiscal Year` = "2018", `Average Daily Popluation` = mean(fy18_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy18_adp))
  
summarize(df_facilities, `Fiscal Year` = "2018", `Average Daily Popluation` = sum(fy18_adp)) %>% pander()
  
  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy18_adp))
  
# fy17_adp
summarize(df_facilities, `Fiscal Year` = "2017", `Average Daily Popluation` = mean(fy17_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy17_adp))
  summarize(df_facilities, `Fiscal Year` = "2017", `Average Daily Popluation` = sum(fy17_adp)) %>% pander()

  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy17_adp))
  
# fy16_adp
summarize(df_facilities, `Fiscal Year` = "2016", `Average Daily Popluation` = mean(fy16_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy16_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2016", `Average Daily Popluation` = sum(fy16_adp)) %>% pander()

  
  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy16_adp))
  
# fy15_adp
summarize(df_facilities, `Fiscal Year` = "2015", `Average Daily Popluation` = mean(fy15_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy15_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2015", `Average Daily Popluation` = sum(fy15_adp)) %>% pander()

  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy15_adp))
  
# fy14_adp
summarize(df_facilities, `Fiscal Year` = "2014", `Average Daily Popluation` = mean(fy14_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy14_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2014", `Average Daily Popluation` = sum(fy14_adp)) %>% pander()

  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy14_adp))
  
# fy13_adp
summarize(df_facilities, `Fiscal Year` = "2013", `Average Daily Popluation` = mean(fy13_adp))%>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy13_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2013", `Average Daily Popluation` = sum(fy13_adp))%>% pander()
  
  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy13_adp))

# fy12_adp
summarize(df_facilities, `Fiscal Year` = "2012", `Average Daily Popluation` = mean(fy12_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy12_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2012", `Average Daily Popluation` = sum(fy12_adp)) %>% pander()

  
  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy12_adp))
  
# fy11_adp
summarize(df_facilities, `Fiscal Year` = "2011", `Average Daily Popluation` = mean(fy11_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy11_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2011", `Average Daily Popluation` = sum(fy11_adp)) %>% pander()

  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy11_adp))
  
# fy10_adp
summarize(df_facilities, `Fiscal Year` = "2010", `Average Daily Popluation` = mean(fy10_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy10_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2010", `Average Daily Popluation` = sum(fy10_adp)) %>% pander()

  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy10_adp))
  
# fy09_adp
summarize(df_facilities, `Fiscal Year` = "2009", `Average Daily Popluation` = mean(fy09_adp)) %>% pander()
  # calculate summary statistics
  summary(select(df_facilities, fy09_adp))
  
  summarize(df_facilities, `Fiscal Year` = "2009", `Average Daily Popluation` = sum(fy09_adp)) %>% pander()
  
  # plot levels
  ggplot(df_facilities) + 
    geom_histogram(mapping = aes(x = fy09_adp))
```

The average daily population for each detention facility is available for `r df_facilities %>% summarise(count = n_distinct(adp))`, respectively.

The probability distributions of the average daily population for the facilities in every year is skewed right. That is, in every year, there are many more facilities that hold very few detainees than there are few facilities that hold many detainees. However, the majority of detention facilities hold zero prisoners in a fiscal year. As a result, the "average" average daily population is XXX.

### Collapse data frame to a single row by group
There are three steps to prepare these data. First, I group facilities by area of responsibility (AOR). Second, I summarize across AORs to compute average daily population, number of facilities, and proportion of facility types. 

```{r df_aor, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}

#%>% 
#   select(aor, detloc, city, zip, everything()) %>%
#   arrange(aor, detloc, city, zip)

  #df_facilities %>% count(n_distinct(aor))
  # unique(df_facilities$aor)
  # count number of AORs
 # df_facilities %>% group_by(aor) %>% tally() %>% pander()

# # aggregate facilities to AOR
# df_aor <- df_facilities %>%
#   group_by(aor) %>% 
#   mutate(
#     n_detloc = n_distinct(detloc), # number of facilities
#     n_city = n_distinct(city), # number of unique cities
#     n_county = n_distinct(county), # number of unique counties
#     n_state = n_distinct(state), # number of unique states 
#     n_zip = n_distinct(zip)) %>% # number of unique zip codes
#   group_by(aor, n_detloc, n_city, n_county, n_state, n_zip) %>%
#   summarize_at(vars(ends_with("adp"), -contains("bookin")), sum) %>%
#   ungroup()


# aggregate facilities to AOR
df_aor <- df_facilities %>%
  group_by(aor) %>% 
  summarize_at(vars(ends_with("adp"), -contains("bookin")), sum) %>%
  ungroup()
```

## Tidy data 
### Reshape data wide to long

Now that I have the ICE facility-level data loaded into `R` and processed to be technically correct, I create an additional data frame that reshapes the data from wide to long. I need to set up the data so they are in both wide and long formats for different parts of these analyses. In the wide format, the repeated responses for an observation (e.g., facility) will be in a single row, and each response is in a separate column. In the long format, each row is one time point per observation, so each facility will have data in multiple rows. There are at least three reasons for structuring data. First, software requirements. For example, the wide format is required for software to execute MANOVA and repeated measures procedures. Likewise, mixed models and many survival analysis procedures require data to be in the long format. Second, analytical implications. For instance, in the wide format, the unit of analysis is the facility whereas each measurement occasion for each facility is the unit of analysis in the long format. Third, the practical difference is that when the occasion is the unit of analysis (i.e., long), I can use each decade’s college education rate as a covariate for the same decade’s jobs value. In the wide format, when the unit of observation is the facility, there is no way to do this. I can use any of the college rates as covariates for all years, but I cannot have decade-specific covariates.

The facility-level data frame is structured to be wide.

Longitudinal data are especially useful for understanding change over time within an AOR, county, city, and facility. 

Such data are structured in long-format, requiring a particular set of statistical techniques to remove bias associated with interdependence of repeated measures. 

Data in long format differ from data in wide format with regard to their analytical requirements. 

At the conclusion of wranging these data, I will be able to provide both cross-sectional and longitudinal analyses of ICE detention facilities.

```{r pivoting_df_aor_long, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
df_aor_long <- df_aor %>% 
  pivot_longer(
    cols = starts_with("fy"),
    names_to = "key",
    values_to = "cases",
  ) %>%
  separate(key, 
           c("fiscal_year", 
             "adp"), 
           sep = "_"
           ) %>% 
  pivot_wider(
    names_from = c(adp), 
    values_from = cases)

# convert to factor
df_aor_long <- df_aor_long %>%
  mutate(fiscal_year = as.factor(fiscal_year))
class(df_aor_long$fiscal_year) 

# recode variable values
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy18'] <- '2018'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy17'] <- '2017'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy16'] <- '2016'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy15'] <- '2015'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy14'] <- '2014'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy13'] <- '2013'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy12'] <- '2012'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy11'] <- '2011'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy10'] <- '2010'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy09'] <- '2009'

# plot levels
  ggplot(df_aor_long) + 
    geom_histogram(mapping = aes(x = adp))
  
# plot adp by fiscal year across aor
ggplot(data = df_aor_long,
       aes(x = fiscal_year, y = adp, group = aor, color = aor)) + 
  geom_line() +
  xlab("Fiscal Year") +
  ylab("Average Daily Population (ADP)") +
 # ggtitle("Average Daily Population (ADP), 2009-2018") +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# plot adp by fiscal year across aor facet wrap 
ggplot(data = df_aor_long,
       aes(x = fiscal_year, y = adp, group = aor, color = aor)) + 
  geom_line() +
  xlab("Fiscal Year") +
  ylab("Average Daily Population (ADP)") +
 # ggtitle("Average Daily Population (ADP), 2009-2018") +
  theme_bw() + 
  facet_wrap(~ aor) + 
  theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# # plot
# ggplot(data = df_aor_long, aes(x = fiscal_year, y = adp, group = aor)) + 
#   geom_line() + 
#   geom_line(stat = "smooth", method = "loess", 
#             aes(group = aor)) + 
#   facet_wrap(~ aor)
```

I explore the distribution of each of the average daily population (ADP) variables in the ICE facilities dataset. For continuous variables, we can measure the centrality and the spread.  Measures of centrality are mean, median. Measures of spread are standard deviation, quantiles, min and max, range, interquartile range.

Second, I create new variables with functions of existing variables (mutate()).

Third, I collapse many values down to a single summary (summarise()). I use these in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group.

summarise() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied exactly the same code to a data frame grouped by date, we get the average delay per date:



Because our unit of analysis is the AOR, I change the scope of each function from operating on the entire dataset to operating on it group-by-group. That is, I collapse the many facility-level values down to a single summary at the AOR-level and by year.


I reshape the wide AOR-level data to long.

```{r pivoting_df_facilities_long, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

# pivot wide to long
df_facilities_long <- df_facilities %>%
  pivot_longer(
    cols = starts_with("fy"),
    names_to = "key",
    values_to = "cases",
  ) 

%>% 
  separate(key, 
           c("fiscal_year", 
             "adp"), 
           sep = "_"
           ) %>% 
  group_by(aor) %>%
  mutate(row = row_number()) %>% 
  pivot_wider(names_from = c(adp), 
              values_from = cases) %>%
  select(-row) 
View(df_aor_long)

# convert to factor
df_aor_long <- df_aor_long  %>%
  mutate(fiscal_year = as.factor(fiscal_year))
class(df_aor_long$fiscal_year) 

# recode variable values
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy18'] <- '2018'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy17'] <- '2017'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy16'] <- '2016'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy15'] <- '2015'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy14'] <- '2014'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy13'] <- '2013'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy12'] <- '2012'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy11'] <- '2011'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy10'] <- '2010'
levels(df_aor_long$fiscal_year)[levels(df_aor_long$fiscal_year)=='fy09'] <- '2009'

# calculate summary statistics
  summary(select(df_aor_long, adp))
  
  # plot levels
  ggplot(df_aor_long) + 
    geom_histogram(mapping = aes(x = adp))
```

Arrests are individual-level data with variables identifying AOR as well as apprehension date. I will summarize arrests by year within each AOR. I will then merge the arrests data with data on ADP by AOR. 

```{r arrests_df_aor, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# aggregate arrests to AOR-level
View(df_arrests)

# separate apprehension date into month, day, and year
df_arrests <- df_arrests %>% 
  separate(apprehension_date, into = c("apprehension_month", "apprehension_day", "apprehension_year")) %>%
  rename(fiscal_year = apprehension_year)

# collapse arrests to summarize aor
df_arrests_aor <- df_arrests %>%
  group_by(aor, fiscal_year) %>% 
  summarize(n_arrests = n()) %>%
  ungroup()
View(df_arrests_aor)

# join aor-level adp and arrests data
newdf <- df_aor_long %>%
  left_join(df_arrests_aor, 
            by = c("aor", "fiscal_year"))

# convert to factor
newdf <- newdf %>%
  mutate(aor = as.factor(aor), 
         fiscal_year = as.factor(fiscal_year)
         )

# calculate summary statistics
summary(newdf$n_arrests)


# plot density curve
ggplot(newdf, aes(x = n_arrests)) + geom_density()
ggplot(newdf, aes(x = log(n_arrests))) + geom_density()

# plot bar graph
ggplot(newdf, aes(x=n_arrests)) + 
  geom_histogram(colour="black", fill="white")

  
ggplot(newdf, aes(x=log(n_arrests))) +
    geom_histogram(colour="black", fill="white")

# plot levels
  ggplot(newdf) + 
    geom_histogram(mapping = aes(x = n_arrests), binwidth = 300, na.rm = TRUE)
  
# plot adp by fiscal year across aor
ggplot(data = newdf,
       aes(x = fiscal_year, y = n_arrests, group = aor, color = aor)) + 
  geom_line() +
  xlab("Fiscal Year") +
  ylab("Arrests") +
 # ggtitle("Average Daily Population (ADP), 2009-2018") +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))

# plot adp by fiscal year across aor facet wrap 
ggplot(data = newdf,
       aes(x = fiscal_year, y = n_arrests, group = aor, color = aor)) + 
  geom_line() +
  xlab("Fiscal Year") +
  ylab("Arrests") +
 # ggtitle("Average Daily Population (ADP), 2009-2018") +
  theme_bw() + 
  facet_wrap(~ aor) + 
  theme(axis.text.x = element_text(angle = 90, size = 8, vjust = .5))




# plot association between adp and arrests
newdf %>%
  # filter(!is.na(adp) & !is.na(n_arrests)) %>% 
    ggplot(aes(x = adp, y = n_arrests)) +
      geom_point() + 
      geom_smooth()

# plot again after removing outliers
newdf %>%
  filter(!is.na(adp) & !is.na(n_arrests) & adp < 5000) %>% 
    ggplot(aes(x = adp, y = n_arrests)) +
      geom_point(na.rm = TRUE) + 
      geom_smooth()


ggplot(newdf, aes(adp, n_arrests)) + geom_boxplot()



# https://murraylax.org/rtutorials/regression_intro.html


mod <- lm(log(n_arrests) ~ log(adp), data = newdf)

mod$coefficients

ggplot(data = newdf, mapping = aes(x = adp, y = n_arrests)) + 
  geom_point(position = "jitter") + 
  labs(title = "Effects of ADP on Arrests",
       x = "Average Daily Population",
       y = "Arrests") + 
  geom_smooth(method = "lm") + 
  theme_bw()

newdf2 <- newdf %>%
  add_residuals(mod) %>%
  mutate(resid = exp(resid))

ggplot(data = newdf2) + 
  geom_point(mapping = aes(x = adp, y = resid))
```
The v






















# aggregate arrests to AOR and year
arrests_df_aor <- test %>%
  group_by(aor, year) %>% 
  mutate(n_apprehension_method = n_distinct(apprehension_method),
         n_apprehension_landmark = n_distinct(apprehension_landmark),
         n_proccessing_disposition = n_distinct(processing_disposition), 
         n_citizenship = n_distinct(citizenship),
         n_gender = n_distinct(gender)) %>%
  group_by(aor, year, n_apprehension_method, n_apprehension_landmark, n_proccessing_disposition, n_citizenship, n_gender) %>%
  summarize_at(vars(contains("aor"))) %>% 
  ungroup()



# summarize number
# aor
# check levels
class(df_arrests$aor)

df_arrests %>%
  count(aor)

# verify unique IDs
df_arrests %>%
  count(aor) %>%
  filter(n > 1) # rows are unique except for "redacted" obs (n = 207)
                # redacted obs are type == "ORR"

df_arrests %>%
  summarise(count = n_distinct(aor)) # 1479 unique obs


levels(as.factor(df_arrests$aor))
"ATL" Atlanta
"BAL" Baltimore
"BOS" Boston
"BUF" Buffalo
"CHI" Chicago
"DAL" Dallas
"DEN" Denver
"DET" Detroit
"ELP" El Paso
"HOU" Houston
"HQ" 
"LOS" Los Angeles Area of Responsibility: Los Angeles Metropolitan Area (Counties of Los Angeles, Orange, Riverside, San Bernardino), and Central Coast (Counties of Ventura, Santa Barbara and San Luis Obispo)
"MIA" Miami
"NEW" Newark
"NOL" New Orleans
"NYC" New York City
"PHI" Philadelphia
"PHO" Phoenix Area of Responsibility: Arizona
"SEA" Seattle
"SFR" San Francisco
"SLC" Salt Lake City
"SNA" San Antonio
"SND" San Diego
"SPM" St Paul
"WAS" Washington (VA)


df_arrests %>% 
  filter(detloc != "Redacted") %>% # filter out redacted obs
  summarise(count = n_distinct(detloc)) # 1478 unique obs

# convert to factor
df_arrests <- df %>%
  mutate(detloc = as.factor(detloc))

df_arrests %>%
  count(aor, apprehension_date) 



```

With the facility-level data structured in both long and wide formats, we can start to describe univariate statistics and covariation between variables of interest using numerical summaries and graphical displays.

The following analyses describes the ICE facilities dataset, which includes both continuous and categorical variables. A variable is continuous if it can take any of an infinite set of ordered values (e.g., income). There are several different plots that can effectively communicate the different features of continuous variables. Features I am generally interested in include measures of location and spread, symmetry, and outliers. In contrast, a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property (e.g., race-ethnicity, gender, citizenship). There are a few different plots that can effectively communicate features of categorical variables. The features of these variables in which I am generally interested cover counts and proportions.



# Exploratory Data Analysis (EDA)
The CMTO project collects data on the characteristics of both households and individuals within those households. This survey includes information on facility location, household composition and characteristics, and socioeconomic characteristics. First, the geographic context in which households are situated is determined. Second, households within these geographies are characterized by wealth, size, number of children, age and sex composition and structure, and expenditures on rental units. Third, individual-level indicators provide additional characteristics of household head, children, and other household members. These data on individuals are collected on variables such as race-ethnicity, gender, age, homeless status, disability status, relationship to the household head. 

The analytical framework implemented in this analysis illustrates these interrelationships. This framework can be used to analyze data on households' outcomes in economic attainment, educational achievement, and access to geographic opportunity by a great many demographic, housing, and socioeconomic indicators.

## Sample selection
Two questions I will be answering in this section are:

* What type of variation occurs within my variables?

* What type of covariation occurs between my variables?

The current anal ysis is interested in the characteristics of AORs in 2011 through 2019. For this purpose, I aggregate characteristics of facilities before recalculating summary statistics.

The full sample, however, is much larger. The total number of participants and households is `r format(df.demog %>% summarise(count = n_distinct(member_ssn)), big.mark = ",")` and `r df.demog %>% summarise(count = n_distinct(entityid))`, respectively.

```{r adp_by_year, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}

# # compute rate per 10,000
# table1 %>% 
#   mutate(rate = cases / population * 10000)

# visualize changes over time
aor_by_year <- ggplot(data = df_aor_long, 
       aes(x = fiscal_year, y = adp, 
           group = aor)) +
  geom_line() +
  xlab("Year") + 
  ylab("Average Daily Population") +
  ggtitle("Average Daily Population, 2011--2019") +
  theme_bw(base_size=8) + 
  theme(axis.text.x = element_text(angle = 90, size = 10)) #+
 # facet_wrap(~ aor)

aor_by_year
```


```{r cert_type, echo=FALSE, results='hide', warning=FALSE, message=FALSE, include=FALSE}

# cert_type
# check levels
class(df$cert_type)

df %>%
  count(cert_type)
table(df$cert_type)

# plot levels
ggplot(df, aes(cert_type)) + 
  geom_bar()  +
  theme(axis.text.x = element_text(angle = 90, size = 10)) + 
  facet_grid()

# check proportions
table(df$cert_type)
tab1 <- table(df$cert_type)
prop.table(tab1)

# generate proportion variable
df <- df %>% 
  left_join(
    df %>% 
      group_by(cert_type) %>% 
      summarize(n = n()) %>% 
      mutate(cert_type_prop = prop.table(n)) # prop = n / sum(n) works too
    )
df$n <- NULL

# create a reusable ggplot object
(ggplot(df, aes(cert_type)) + 
  geom_bar(fill = 'blueviolet') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, size = 10, vjust = .5)) +
  labs(x = 'Certification Type', y = 'Count',
       title = 'Number of Participants by Certification Type'))
```

Together, the sample of `new admissions` and `issuances of vouchers` contains approximately `r round((0.363+0.505)*100)` percent of the full CMTO sample. Because these descriptive analyses are interested in the attributes of both households as well as youth according to eligibility requirements, I design two samples subsetting household heads and their school-aged children between 15 and 18 years old. The bar chart below illstrates household composition across the full sample by certification type.

```{r member_relation_cert_type_vis, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# new admission and issuance of voucher subsample
df.newadmission.issuance <- df %>% 
  filter(cert_type == "Issuance of Voucher" | 
           cert_type == "New Admission")

ggplot(df.newadmission.issuance, 
       aes(cert_type)) + 
  geom_bar(aes(fill = member_relation)) +
  scale_fill_brewer(palette="Spectral") +
  theme(axis.text.x = element_text(angle = 45, size = 10, vjust = .5)) +
  labs(x = 'Certification Type', y = 'Count',
       title = 'Number of Participants by Certification Type and Relationship to Household Head') 
```

Across samples, the majority of individuals are considered heads of households and youth under the age of 18.

```{r newadmission_subsamples, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# full new admission subsample
 df.demog.newadmission <- df %>%
  filter(cert_type == "New Admission")

# household-level subsample
      df.demog.newadmission.hoh <- df %>%
        filter(cert_type == "New Admission" & member_relation == "Head Of Household")
        # verify unique IDs
          df.demog.newadmission.hoh %>%
            count(entityid) %>%
            filter(n > 1)# one duplicate entityid == 127429
                    df.test <- df.demog.newadmission.hoh %>% filter(entityid == "127429")
    
# child-level subsample
    df.demog.newadmission.child <- df %>%
      filter(cert_type == "New Admission" &
               age_num == 15:18)
        # verify unique IDs
          df.demog.newadmission.child %>%
            count(member_ssn) %>%
            filter(n > 1)  # no duplicates
```
By counting the number of rows in the new admissions data frame, the total number of survey participants is ``r df.demog.newadmission %>% summarise(count = n_distinct(member_ssn))`` and the total number of households represented in the data frame is `r df.demog.newadmission %>% summarise(count = n_distinct(entityid))`. 

```{r }
#df.demog.w.ind %>% 
#  filter(age_num == 15) %>% 
#  count(n_distinct(entityid_time1))

#names(df.demog.w.ind)

#df %>%
#  filter(cert_type == "New Admission") %>% 
#  summarise(count = n_distinct(entityid))
```

Of the new admissions, a total of `r df.demog.newadmission.child %>% summarize(count = n_distinct(entityid))` households have children aged between the years of 15 and 18.

```{r issuance_subsamples, echo=FALSE, results='hide', warning=FALSE, message=FALSE}

# full issuance of voucher subsample
 df.demog.issuance <- df %>% 
   filter(cert_type == "Issuance of Voucher")
 
# household-level subsample
       df.demog.issuance.hoh <- df %>%
        filter(cert_type == "Issuance of Voucher" & member_relation == "Head Of Household")
        # verify unique IDs
          df.demog.issuance.hoh %>%
            count(entityid) %>%
            filter(n > 1)
          
# child-level subsample
     df.demog.issuance.child <- df %>%
       filter(cert_type == "Issuance of Voucher" & 
                age_num == 15:18)
        # verify unique IDs
          df.demog.issuance.child %>%
            count(member_ssn) %>%
            filter(n == 1) 
```
By contrast, the data frame subsetting on issuance of vouchers has a total number of participants equal to `r df.demog.issuance %>% summarise(count = n_distinct(member_ssn))`. The total number of households in this truncated dataset amounts to `r df.demog.issuance %>% summarise(count = n_distinct(entityid))`. For households issued vouchers, `r df.demog.issuance.child %>% summarize(count = n_distinct(entityid))` have youth who are between 15 and 18 years of age.

I tidy these data to then facilitate their transformation and visualization. I describe univariate statistics and covariation between variables I observe for the purpose of modelling associations among immigration detention facilities, interior immigration law enforcement practices, and immigration bail bonds.


## Summarizing and visualizing distributions

It is important to be able to understand how an individual variable is distributed before moving on to more sophisticated visualizations that enable multidimensional investigation. Visually understanding the distribution allows us to describe many features of a variable.

### Variation
I first present several numerical summaries and create graphical displays showing the univariate distributions of different variables that are of interest to the Human Rights at Home project .

### Covariation
Bivariate Relationships and Associations
Having a solid understanding of univariate distributions is important; however, most analyses want to take the next step to understand associations and relationships between variables. Features we are generally interested in include:

Associations
Outliers
Clusters
Gaps
Barriers
Change points








```{}
# FOR INCOME VARIABLES WHERE I CAN'T SEE THE FULL DISTRIBUTION BECAUSE THE RANGE IS TOO WIDE
ggplot(df.demog.newadmission) + 
  geom_histogram(mapping = aes(x = age_num), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 20))
```



```{r newadmission_age_num_race_vis, eval=TRUE, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# plot levels
g <- ggplot(df.demog.newadmission, aes(age_num))
g + geom_density(aes(fill = race_ethnicity), alpha=0.8) + 
    labs(title = "Distribution of Age by Race-ethnicity", 
         subtitle = "New Admissions",
         y = "Probability per age (years)",
         x = "Age (years)",
         fill = "Race-ethnicity")
```

Kernel density plots present the probability of an individual occurring within a particular age. This density plot suggests the population of Native Americans and multi-racial individuals in the CMTO sample of new admissions is particularly y oung. Put differently, the probability that these two ethnoracial groups are represented in children aged 10 is very high. Latinos are overrepresented in the age range between 20 and 30 years. In addition to residents who identified as American Indians, white residents have a high probability of being between the ages of 30 and 40 years. It also appears that the eldest participants among new admissions are Native Hawaiian.

```{r issuance_age_num_race_vis, eval=TRUE, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# plot levels
g <- ggplot(df.demog.issuance, aes(age_num))
g + geom_density(aes(fill = race_ethnicity), alpha=0.8) + 
    labs(title = "Distribution of Age by Race-ethnicity", 
         subtitle = "Issuance of Vouchers",
         #caption="Source: mpg",
         y = "Probability per age (years)",
         x = "Age (years)",
         fill = "Race-ethnicity")
```

The distribution of age by ethnoracial affiliation is different among the sample of issued vouchers. For example, the probability that a person who identifies as multiracial is 10-years-old is much higher than in the sample of new admissions. Asian participants also appear to be older in the sample of issued vouchers when compared to the sample of new admissions. Latinos and Native Americans have similar probabilities around 10-years-old within this sample.

```{r}
#Dates confirm voucher-holding households moved-in, while issuance of vouchers suggests households are in the process of searching for rental housing. 

#The duration between these dates prompts important questions related to the housing search process, residential mobility, racial equity, and social justice.
```

```{r newadmission_hoh_single_adult_race_vis, eval=TRUE, echo=FALSE, results='hide', warning=FALSE, message=FALSE}


# plot levels
ggplot(df.demog.newadmission.hoh, 
       aes(single_adult)) + 
  geom_bar(aes(fill = race_ethnicity)) +
  scale_fill_brewer(palette="Accent") +
  theme(axis.text.x = element_text(angle = 90, size = 10, vjust = .5)) +
  labs(x = 'Single-headed household', y = 'Count',
       title = 'Number of Households with Single Adults by Race-Ethnicity',
       subtitle = 'New Admissions')

#How many CMTO participants are elderly?
#How many CMTO participants have a disability?
#How many CMTO participants are single-headed families with children?
```  
  
```{r newadmission_disability_age_num_vis, eval=TRUE, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# generate binary variable of age
df.demog.newadmission <- df %>% 
  mutate(age_num_cat = as.factor(ifelse(age_num %in% 0:17, "Under-18 years old",
                                     ifelse(age_num %in% 18:100, "18+ years old",
                                            NA))))

# plot levels
ggplot(subset(df.demog.newadmission, member_disability == "Yes"), 
       aes(member_disability)) + 
  geom_bar(aes(fill = age_num_cat), position = 'dodge') +
  scale_fill_brewer(palette="Pastel1") +
  theme(axis.text.x = element_text(size = 10, vjust = .5)) +
  labs(x = 'Disability Status', y = 'Count',
       title = 'Number of Households by Disability Status and Age') 
```

```{r issuance_disability_age_num_vis, eval=TRUE, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# generate binary variable of age
df.demog.issuance <- df %>% 
  mutate(age_num_cat = as.factor(ifelse(age_num %in% 0:17, "Under-18 years old",
                                     ifelse(age_num %in% 18:100, "18+ years old",
                                            NA))))

# plot levels
ggplot(subset(df.demog.issuance, member_disability == "Yes"), 
       aes(member_disability)) + 
  geom_bar(aes(fill = age_num_cat)) +
  scale_fill_brewer(palette="Pastel1") +
  theme(axis.text.x = element_text(angle = 45, size = 10, vjust = .5)) +
  labs(x = 'Disability Status', y = 'Count',
       title = 'Number of Households by Disability Status and Age',
       subtitle = 'Issuance of Vouchers')

```

3. Future research questions:
"After being picked up by ICE officials, either at the Border, through Secure Communities, or by local officials by way of Section 287(g), an individual can be released on bond if they are not deemed a "threat" to national security."


  
  mpg %>%
    ggplot(aes(cty)) +
    geom_histogram(binwidth = 1.25, color = "black",fill = "grey") +
    geom_vline(xintercept = mean(mpg$cty), lwd = 2) +
    labs(title = "Distribution of cty",
         x = "cty",
         y = "Number of cars") +
    theme_minimal() +
    scale_x_continuous(breaks = seq(7.5,35,2.5))

  
  
  
  Bivariate analysis of a continuous variable with respect to a categorical variable
Before we start our calculations, let’s continue in a graphical way. Before, we have analyzed the distribution of cty, as shown with the histogram above. Now we want to analyse it for different types of drivings (e.g. 4-wheel drive, front wheel drive or rear wheel drive), as recorded by the variable drv. Graphically, we can plot different histograms for each of these three categories using facets.

mpg %>%
    ggplot(aes(cty)) +
    geom_histogram(binwidth = 1.25, color = "black",fill = "grey") +
    labs(title = "Distribution of cty relative to drv",
         x = "cty",
         y = "Number of cars") +
    theme_minimal() +
    scale_x_continuous(breaks = seq(7.5,35,2.5)) +
    facet_grid(drv~.)
    
    
    
    
    ** a. What is the influence of average daily population (ADP) on:
***  i. encounters
***  ii. arrests
*** iii. removals

**a. What is the influence of detention center capacity on:
***  i. money deposit
***  ii. cash bail
    
    Enforcement index by Area of Responsibility (AOR):
Total base capacity for detention centers
Total number of “as needed” centers
Total number of facilities
Facility type (dummy variable)
County (there are other categories)
Private
Bureau of Prisons (federal)
Guaranteed minimum (dummy variable)
Additional variables:
Border
Southern border (1,0)
Northern border (1,0)

I often hear concern about the non-normal distributions of independent variables in regression models, and I am here to ease your mind.

There are NO assumptions in any linear model about the distribution of the independent variables.  Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  But no, the model makes no assumptions about them.  They do not need to be normally distributed or continuous.

It is useful, however, to understand the distribution of predictor variables to find influential outliers or concentrated values.  A highly skewed independent variable may be made more symmetric with a transformation.